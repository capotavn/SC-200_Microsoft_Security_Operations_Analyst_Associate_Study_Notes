# SC-200 Study Notes - Module 5: Microsoft Sentinel (Part 2)
## ğŸ›¡ï¸ Data Collection, Threat Intelligence & Cost Optimization

**Continuation of Part 1** - Sections 4.3-8
**Last Updated:** October 2025
**Based on:** Official SC-200 Study Guide + Latest Sentinel Updates

---

## 4.3 Agent-Based Connectors

### Windows Security Events via AMA

**Azure Monitor Agent (AMA) - Modern Agent:**

```
What: Next-generation data collection agent (replaces Log Analytics Agent)

AMA vs Legacy Agent (MMA):

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Feature          | Legacy Agent (MMA) | AMA           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Status           | Deprecated (2024)  | Current (GA)  â”‚
â”‚ OS Support       | Windows, Linux     | Windows, Linuxâ”‚
â”‚ Configuration    | Workspace-based    | DCR-based     â”‚
â”‚ Filtering        | Limited            | Advanced      â”‚
â”‚ Multi-homing     | Yes                | Yes           â”‚
â”‚ Performance      | Moderate           | Better        â”‚
â”‚ Management       | Complex            | Simplified    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

âš ï¸ Legacy Agent (MMA/OMS) deprecated August 2024
   â†’ Migrate to AMA before end of support!
```

**Deploying AMA for Windows Security Events:**

```
Configuration Steps:

Step 1: Create Data Collection Rule (DCR)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Azure Portal â†’ Monitor â†’ Data Collection Rules â†’ Create

DCR Configuration:
â”œâ”€ Rule name: DCR-WindowsSecurityEvents-Prod
â”œâ”€ Subscription: Select subscription
â”œâ”€ Resource group: rg-sentinel-prod
â”œâ”€ Region: Same as Sentinel workspace
â”œâ”€ Platform: Windows
â””â”€ Data source: Windows Event Logs

Event Sets (Pre-defined Collections):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Set         | Events/Day | Use Case              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ All Events  | ~5,000+    | Forensics, compliance â”‚
â”‚ Common      | ~300       | Recommended (balance) â”‚
â”‚ Minimal     | ~50        | Basic monitoring      â”‚
â”‚ Custom      | Varies     | Specific Event IDs    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Common Set (Recommended):
- Includes: Logins, process creation, privilege use, firewall changes
- Excludes: Noise (routine events like Group Policy refresh)
- Size: ~300 events/day per machine

Custom XPath Queries (Advanced Filtering):
Example: Only Security Event ID 4625 (Failed Logons)
<QueryList>
  <Query Id="0">
    <Select Path="Security">
      *[System[(EventID=4625)]]
    </Select>
  </Query>
</QueryList>

Step 2: Assign DCR to Resources
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DCR â†’ Resources â†’ Add

Assignment Options:
â”œâ”€ Individual VMs: Select specific machines
â”œâ”€ Resource Group: All VMs in RG
â”œâ”€ Subscription: All VMs in subscription
â””â”€ Azure Policy: Auto-assign to VMs matching criteria (recommended)

Azure Policy Assignment (Recommended):
Policy: "Configure Windows machines to run Azure Monitor Agent"
- Scope: Subscription or Resource Group
- Effect: DeployIfNotExists (auto-installs AMA)
- Remediation: Auto-remediate existing VMs

Step 3: Verify Data Collection
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Wait: 5-10 minutes for first events

Query:
SecurityEvent
| where TimeGenerated > ago(1h)
| where Computer == "SERVER01"
| take 10

Expected: Security events appearing in SecurityEvent table

Step 4: Monitor Collection Health
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Sentinel â†’ Health â†’ Data connectors

Check:
âœ… AMA installed on targets
âœ… DCR assigned correctly
âœ… Events flowing (ingestion rate)
âŒ Errors: Missing permissions, agent offline, DCR misconfigured
```

**Key Security Event IDs (SC-200 Exam Focus):**

```
Critical Windows Security Events:

Account Logon Events:
â”œâ”€ 4624: Successful logon
â”œâ”€ 4625: Failed logon (brute force indicator)
â”œâ”€ 4634: Logoff
â”œâ”€ 4648: Logon with explicit credentials (runas)
â””â”€ 4672: Special privileges assigned (admin logon)

Account Management:
â”œâ”€ 4720: User account created
â”œâ”€ 4722: User account enabled
â”œâ”€ 4723: Password change attempt
â”œâ”€ 4724: Password reset attempt
â”œâ”€ 4725: User account disabled
â”œâ”€ 4726: User account deleted
â”œâ”€ 4728: Member added to security-enabled global group
â”œâ”€ 4732: Member added to security-enabled local group
â””â”€ 4756: Member added to security-enabled universal group

Privilege Use:
â”œâ”€ 4672: Special privileges assigned to new logon
â”œâ”€ 4673: Privileged service called
â””â”€ 4674: Operation attempted on privileged object

Process Tracking:
â”œâ”€ 4688: New process created (PowerShell, cmd, etc.)
â””â”€ 4689: Process exited

Object Access:
â”œâ”€ 4663: Attempt to access object (file, registry)
â”œâ”€ 4656: Handle to object requested
â””â”€ 4660: Object deleted

Policy Change:
â”œâ”€ 4719: System audit policy changed
â”œâ”€ 4739: Domain policy changed
â””â”€ 4670: Permissions on object changed

System Events:
â”œâ”€ 4616: System time changed
â”œâ”€ 4697: Service installed
â”œâ”€ 1102: Audit log cleared (cover tracks)
â””â”€ 7045: Service installed (malware persistence)

Detection Use Cases:
â”œâ”€ Brute force: Multiple 4625 events (>10 in 5 min)
â”œâ”€ Privilege escalation: 4728/4732 (added to admin group)
â”œâ”€ Lateral movement: 4648 (explicit credential use)
â”œâ”€ Persistence: 4697/7045 (service installed)
â”œâ”€ Anti-forensics: 1102 (log cleared)
â””â”€ Suspicious processes: 4688 (PowerShell, wmic, etc.)
```

### Linux Syslog

**Collecting Linux Syslog:**

```
Syslog Connector Architecture:

Linux Machines â†’ rsyslog/syslog-ng â†’ Log Analytics workspace â†’ Sentinel

Configuration Steps:

Step 1: Deploy AMA on Linux
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Install via:
- Azure portal (VM extension)
- Azure Policy (auto-deploy)
- CLI: az vm extension set

Step 2: Create DCR for Syslog
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Azure Portal â†’ Monitor â†’ Data Collection Rules â†’ Create

Configuration:
â”œâ”€ Platform: Linux
â”œâ”€ Data source: Linux Syslog
â”œâ”€ Facilities: Select log sources
â”‚  â”œâ”€ auth (authentication)
â”‚  â”œâ”€ authpriv (SSH, sudo)
â”‚  â”œâ”€ syslog (general system)
â”‚  â”œâ”€ daemon (background services)
â”‚  â”œâ”€ kern (kernel messages)
â”‚  â””â”€ cron (scheduled tasks)
â”‚
â””â”€ Log levels: Select severity
   â”œâ”€ Emergency (0) - system unusable
   â”œâ”€ Alert (1) - immediate action needed
   â”œâ”€ Critical (2) - critical conditions
   â”œâ”€ Error (3) - error conditions
   â”œâ”€ Warning (4) - warning conditions
   â”œâ”€ Notice (5) - normal but significant
   â”œâ”€ Info (6) - informational
   â””â”€ Debug (7) - debug messages

Recommended Configuration:
Facility: authpriv (SSH/sudo) â†’ Log level: Info
Facility: syslog (general) â†’ Log level: Warning
Facility: daemon (services) â†’ Log level: Error

Step 3: Configure rsyslog (on Linux machine)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
File: /etc/rsyslog.d/95-omsagent.conf

*.info;mail.none;authpriv.none;cron.none @127.0.0.1:25224

Restart rsyslog:
sudo systemctl restart rsyslog

Step 4: Verify Collection
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Query:
Syslog
| where TimeGenerated > ago(1h)
| where Computer == "ubuntu-vm-01"
| take 10

Table: Syslog
Columns:
â”œâ”€ Computer: Hostname
â”œâ”€ Facility: Log source (authpriv, syslog, etc.)
â”œâ”€ SeverityLevel: Info, Warning, Error, etc.
â”œâ”€ SyslogMessage: Log message text
â””â”€ ProcessName: Process that generated log
```

**Common Syslog Detection Use Cases:**

```
SSH Brute Force Detection:
Syslog
| where Facility == "authpriv"
| where SyslogMessage contains "Failed password"
| summarize FailedLogins = count() by Computer, bin(TimeGenerated, 5m)
| where FailedLogins > 10

Privilege Escalation (sudo):
Syslog
| where Facility == "authpriv"
| where SyslogMessage contains "sudo"
| where SyslogMessage contains "COMMAND"

Service Failures:
Syslog
| where Facility == "daemon"
| where SeverityLevel == "err"
| where SyslogMessage contains "failed"

Kernel Panics:
Syslog
| where Facility == "kern"
| where SeverityLevel == "emerg" or SeverityLevel == "crit"
```

### 4.4 Syslog / CEF Connectors

**Common Event Format (CEF) for Network Devices:**

```
CEF Architecture:

Firewall/IDS/IPS â†’ Syslog (CEF format) â†’ Linux Forwarder â†’ Sentinel

Why CEF?
- Standardized format (ArcSight format)
- Supported by 100+ security vendors
- Easy parsing (structured fields)

CEF Message Format:
CEF:Version|Device Vendor|Device Product|Device Version|Signature ID|Name|Severity|Extension

Example:
CEF:0|Palo Alto Networks|PAN-OS|8.0|TRAFFIC|end|3|
rt=Jan 01 2025 12:00:00 src=10.0.0.5 dst=203.0.113.50 spt=54321 dpt=443
```

**Deploying CEF Collector:**

```
Step 1: Deploy Linux VM (CEF Forwarder)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Recommended Specs:
â”œâ”€ OS: Ubuntu 18.04/20.04 or RHEL 7/8
â”œâ”€ CPU: 2 vCPUs minimum (4 recommended)
â”œâ”€ RAM: 4 GB minimum (8 GB recommended)
â”œâ”€ Disk: 50 GB
â””â”€ Network: Open ports 514 (TCP/UDP), 25226

Note: Can handle 500+ devices per forwarder

Step 2: Install CEF Collector Script
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Run as root on Linux VM:

wget -O cef_installer.py \
https://raw.githubusercontent.com/Azure/Azure-Sentinel/master/DataConnectors/CEF/cef_installer.py

sudo python cef_installer.py <WorkspaceID> <WorkspaceKey>

Script installs:
- Rsyslog (syslog daemon)
- AMA agent (sends logs to Sentinel)
- CEF parser (parses CEF format)

Step 3: Configure Firewall to Send CEF Logs
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Example: Palo Alto Firewall

1. Go to: Device â†’ Server Profiles â†’ Syslog
2. Create syslog profile:
   - Name: Sentinel-CEF
   - Server: <Linux_Forwarder_IP>:514
   - Transport: TCP
   - Format: BSD
   - Facility: LOG_LOCAL4

3. Go to: Objects â†’ Log Forwarding
4. Create profile:
   - Name: Forward-to-Sentinel
   - Log types: Traffic, Threat, Wildfire
   - Syslog profile: Sentinel-CEF

5. Apply to Security Policies
   - Select policies
   - Actions â†’ Log Forwarding: Forward-to-Sentinel

Step 4: Verify CEF Collection
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Wait: 10-20 minutes for first logs

Query:
CommonSecurityLog
| where TimeGenerated > ago(1h)
| where DeviceVendor == "Palo Alto Networks"
| take 10

Table: CommonSecurityLog (CEF parsed)
Key Columns:
â”œâ”€ DeviceVendor: Vendor name (Palo Alto, Cisco, etc.)
â”œâ”€ DeviceProduct: Product name (PAN-OS, ASA, etc.)
â”œâ”€ DeviceAction: Action taken (allow, deny, alert)
â”œâ”€ SourceIP / DestinationIP: IPs
â”œâ”€ SourcePort / DestinationPort: Ports
â”œâ”€ Protocol: TCP, UDP, ICMP
â”œâ”€ Severity: 0-10 (CEF severity scale)
â””â”€ Message: Original CEF message
```

**Supported CEF Vendors (SC-200 Focus):**

```
Firewalls:
â”œâ”€ Palo Alto Networks (PAN-OS)
â”œâ”€ Cisco ASA, Firepower
â”œâ”€ Fortinet FortiGate
â”œâ”€ Check Point (via Log Exporter)
â””â”€ pfSense / OPNsense

IDS/IPS:
â”œâ”€ Cisco Firepower
â”œâ”€ Fortinet FortiGate IPS
â”œâ”€ Snort
â””â”€ Suricata

WAF (Web Application Firewall):
â”œâ”€ F5 BIG-IP ASM
â”œâ”€ Imperva SecureSphere
â””â”€ Barracuda WAF

Network Security:
â”œâ”€ Zscaler (Internet Access, Private Access)
â”œâ”€ Cisco Umbrella (DNS security)
â””â”€ Cisco Meraki

Endpoint Security:
â”œâ”€ Symantec Endpoint Protection
â”œâ”€ Trend Micro Deep Security
â””â”€ CrowdStrike (via Syslog)

Full list: 100+ vendors in Sentinel documentation
```

**ğŸ¯ Exam Tip:**
- **AMA** = Modern agent (replaced MMA), DCR-based configuration
- **DCR** (Data Collection Rule) = Defines what data to collect, where to send
- **Windows Security Events**: Use "Common" set (balanced), know key Event IDs (4624, 4625, 4688, 1102)
- **Linux Syslog**: Facilities (authpriv, syslog, daemon), severity levels
- **CEF**: Common Event Format (ArcSight), requires Linux forwarder VM
- **CEF vendors**: Palo Alto, Cisco, Fortinet, Check Point, Zscaler
- **Tables**: SecurityEvent (Windows), Syslog (Linux), CommonSecurityLog (CEF)

---

## 5. Data Collection Rules (DCR)

### 5.1 DCR Overview

**What are Data Collection Rules?**

```
Data Collection Rule (DCR):
- Defines: WHAT data to collect, WHERE to send it, HOW to transform it
- Scope: Can apply to multiple resources (VMs, resource groups, subscriptions)
- Format: ARM template (Infrastructure as Code)
- Benefits: Centralized management, version control, consistency

DCR vs Workspace Configuration:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Feature          | Workspace Config | DCR              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Agent            | MMA (legacy)     | AMA (modern)     â”‚
â”‚ Configuration    | Per-workspace    | Reusable         â”‚
â”‚ Filtering        | Limited          | Advanced (XPath) â”‚
â”‚ Transformation   | No               | Yes (KQL)        â”‚
â”‚ Deployment       | Manual           | ARM, Policy      â”‚
â”‚ Multi-workspace  | No               | Yes              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ¯ Exam: Always use DCR (not workspace config) for AMA scenarios
```

### 5.2 DCR Components

**DCR Structure:**

```json
{
  "name": "DCR-WindowsSecurityEvents",
  "location": "eastus",
  "properties": {
    "dataSources": {
      "windowsEventLogs": [
        {
          "streams": ["Microsoft-SecurityEvent"],
          "xPathQueries": [
            "Security!*[System[(EventID=4624 or EventID=4625)]]"
          ],
          "name": "eventLogsDataSource"
        }
      ]
    },
    "destinations": {
      "logAnalytics": [
        {
          "workspaceResourceId": "/subscriptions/.../workspaces/sentinel-prod",
          "name": "sentinelWorkspace"
        }
      ]
    },
    "dataFlows": [
      {
        "streams": ["Microsoft-SecurityEvent"],
        "destinations": ["sentinelWorkspace"]
      }
    ]
  }
}
```

**DCR Components Explained:**

```
1ï¸âƒ£ Data Sources (What to Collect)
   â”œâ”€ Windows Event Logs: Security, Application, System
   â”‚  â””â”€ XPath queries: Filter specific Event IDs
   â”œâ”€ Linux Syslog: Facilities and severity levels
   â”œâ”€ Performance Counters: CPU, memory, disk, network
   â””â”€ Custom Logs: IIS logs, application logs

2ï¸âƒ£ Destinations (Where to Send)
   â”œâ”€ Log Analytics Workspace (primary)
   â”œâ”€ Azure Monitor Metrics (for performance data)
   â””â”€ Azure Event Hubs (for streaming to SIEM)

3ï¸âƒ£ Data Flows (Routing)
   â”œâ”€ Maps: Data source â†’ Destination
   â”œâ”€ Transformation: Apply KQL transformations (filter, enrich)
   â””â”€ Multiple flows: One source â†’ multiple destinations

4ï¸âƒ£ Transformations (ğŸ†• Advanced Feature)
   â”œâ”€ KQL-based: Filter, parse, enrich data
   â”œâ”€ Example: Filter out noisy events before ingestion
   â”œâ”€ Benefit: Reduce ingestion costs (send only relevant data)
   â””â”€ Syntax: KQL query in DCR definition

Example Transformation (Filter noisy Event ID 5158):
source
| where EventID != 5158  // Exclude network connection events
| where Computer !contains "test"  // Exclude test machines
```

### 5.3 Advanced DCR Scenarios

**Scenario 1: Multi-Workspace Data Collection**

```
Use Case: Send same data to multiple workspaces (prod + archive)

DCR Configuration:
"destinations": {
  "logAnalytics": [
    {
      "workspaceResourceId": "/subscriptions/.../workspaces/sentinel-prod",
      "name": "prodWorkspace"
    },
    {
      "workspaceResourceId": "/subscriptions/.../workspaces/sentinel-archive",
      "name": "archiveWorkspace"
    }
  ]
},
"dataFlows": [
  {
    "streams": ["Microsoft-SecurityEvent"],
    "destinations": ["prodWorkspace", "archiveWorkspace"]
  }
]

Benefit:
- Prod workspace: 90-day retention, active investigations
- Archive workspace: 7-year retention, compliance
```

**Scenario 2: Filtering Data at Source (Cost Optimization)**

```
Problem: Too many noisy events (Event ID 5156 - Windows Filtering Platform)
Solution: Filter at DCR (don't ingest unnecessary data)

XPath Query (Filter at Source):
<QueryList>
  <Query Id="0">
    <Select Path="Security">
      *[System[(EventID != 5156)]]
    </Select>
  </Query>
</QueryList>

Alternative: KQL Transformation in DCR
source
| where EventID != 5156
| where EventID != 5158  // Also filter network connections
| where EventID != 5157  // Also filter WFP filter

Cost Savings:
- Before: 10 GB/day (including noisy events)
- After: 3 GB/day (filtered)
- Savings: $5,138/month at $2.46/GB (70% reduction!)
```

**Scenario 3: Enrichment at Collection (Advanced)**

```
Use Case: Enrich events with custom data (business context)

Example: Add "Department" field to events based on computer name

KQL Transformation:
source
| extend Department = case(
    Computer startswith "HR-", "Human Resources",
    Computer startswith "FIN-", "Finance",
    Computer startswith "IT-", "IT",
    Computer startswith "SALES-", "Sales",
    "Unknown"
  )
| extend Region = case(
    Computer endswith "-US", "United States",
    Computer endswith "-EU", "Europe",
    Computer endswith "-APAC", "Asia-Pacific",
    "Unknown"
  )

Benefit:
- Enrichment at ingestion (no query-time overhead)
- Enables filtering by Department/Region in analytics rules
- Better for compliance reporting (group by department)
```

**ğŸ¯ Exam Tip:**
- **DCR** = Data Collection Rule (AMA configuration, ARM template)
- **Components**: Data sources (what), Destinations (where), Data flows (routing), Transformations (how)
- **Filtering**: XPath queries (Windows Event Logs), KQL transformations (all data types)
- **Multi-workspace**: Send same data to multiple workspaces
- **Cost optimization**: Filter noisy data at source (before ingestion)
- **Advanced**: KQL transformations for enrichment, filtering, parsing

---

## 6. Custom Log Tables

### 6.1 Custom Logs Overview

**When to Use Custom Logs:**

```
Scenarios:
âœ… Application logs (not supported by built-in connectors)
âœ… Custom security tools (proprietary format)
âœ… Legacy systems (no standard syslog/CEF support)
âœ… IoT devices (custom telemetry)
âœ… Third-party SaaS (via REST API)

Custom Log Sources:
â”œâ”€ Text files: IIS logs, Apache logs, application logs
â”œâ”€ JSON files: Structured application logs
â”œâ”€ CSV files: Custom exports
â”œâ”€ REST API: Any system with API (via Logic Apps)
â””â”€ Streaming: Event Hubs, Kafka
```

### 6.2 HTTP Data Collector API

**Ingesting Custom Logs via REST API:**

```
API Endpoint:
POST https://<WorkspaceID>.ods.opinsights.azure.com/api/logs?api-version=2016-04-01

Headers:
- Content-Type: application/json
- Log-Type: MyCustomLog (table name without _CL suffix)
- Authorization: SharedKey <WorkspaceID>:<Signature>
- x-ms-date: RFC1123 date
- time-generated-field: Timestamp field name (optional)

Body (JSON):
[
  {
    "Timestamp": "2025-10-23T10:30:00Z",
    "Computer": "WEB-01",
    "EventID": 1001,
    "Message": "Application started successfully",
    "User": "john@contoso.com"
  },
  {
    "Timestamp": "2025-10-23T10:31:00Z",
    "Computer": "WEB-01",
    "EventID": 1002,
    "Message": "User authenticated",
    "User": "jane@contoso.com"
  }
]

Response:
200 OK (ingestion successful)

Custom Table Created:
â””â”€ MyCustomLog_CL (note _CL suffix automatically added)

Columns:
â”œâ”€ TimeGenerated: Ingestion time (auto-added)
â”œâ”€ Timestamp: Custom timestamp (from JSON, if specified)
â”œâ”€ Computer_s: String field
â”œâ”€ EventID_d: Double (numeric field)
â”œâ”€ Message_s: String field
â””â”€ User_s: String field

Note: Suffix conventions:
- _s: String
- _d: Double (number)
- _b: Boolean
- _g: GUID
- _t: DateTime
```

**Sample PowerShell Script (HTTP Data Collector):**

```powershell
# Parameters
$WorkspaceId = "12345678-1234-1234-1234-123456789012"
$WorkspaceKey = "base64encodedkey=="
$LogType = "MyCustomLog"

# Build JSON payload
$json = @"
[
  {
    "Timestamp": "$(Get-Date -Format o)",
    "Computer": "$env:COMPUTERNAME",
    "EventID": 1001,
    "Message": "PowerShell event logged",
    "User": "$env:USERNAME"
  }
]
"@

# Build authorization signature
$method = "POST"
$contentType = "application/json"
$resource = "/api/logs"
$rfc1123date = [DateTime]::UtcNow.ToString("r")
$contentLength = $json.Length

$xHeaders = "x-ms-date:" + $rfc1123date
$stringToHash = $method + "`n" + $contentLength + "`n" + $contentType + "`n" + $xHeaders + "`n" + $resource

$bytesToHash = [Text.Encoding]::UTF8.GetBytes($stringToHash)
$keyBytes = [Convert]::FromBase64String($WorkspaceKey)
$sha256 = New-Object System.Security.Cryptography.HMACSHA256
$sha256.Key = $keyBytes
$calculatedHash = $sha256.ComputeHash($bytesToHash)
$encodedHash = [Convert]::ToBase64String($calculatedHash)
$authorization = 'SharedKey {0}:{1}' -f $WorkspaceId,$encodedHash

# Build headers
$headers = @{
    "Authorization" = $authorization
    "Log-Type" = $LogType
    "x-ms-date" = $rfc1123date
}

# Send to Sentinel
$uri = "https://" + $WorkspaceId + ".ods.opinsights.azure.com" + $resource + "?api-version=2016-04-01"
Invoke-RestMethod -Uri $uri -Method Post -ContentType $contentType -Headers $headers -Body $json

Write-Host "Custom log sent to Sentinel successfully!"
```

### 6.3 Custom Table Schema

**Designing Custom Tables:**

```
Best Practices:

1ï¸âƒ£ Table Naming
   â”œâ”€ Use descriptive names: AppName_EventType_CL
   â”œâ”€ Examples: WebApp_Login_CL, Firewall_Deny_CL
   â”œâ”€ Avoid: Generic names (CustomLog_CL, Data_CL)
   â””â”€ Note: _CL suffix automatically added

2ï¸âƒ£ Field Naming
   â”œâ”€ Use consistent naming: PascalCase or snake_case
   â”œâ”€ Good: UserPrincipalName, source_ip_address
   â”œâ”€ Avoid: usr, src, data1 (not descriptive)
   â””â”€ Include: Timestamp, Computer, User (standard fields)

3ï¸âƒ£ Data Types
   â”œâ”€ Use appropriate types: String, Double, Boolean, DateTime
   â”œâ”€ Sentinel auto-detects: Based on first values ingested
   â”œâ”€ Problem: If first value is "123", detected as Double (not String!)
   â””â”€ Solution: Send sample with all data types first (schema definition)

4ï¸âƒ£ Schema Evolution
   â”œâ”€ Adding fields: Automatic (new fields added on ingestion)
   â”œâ”€ Changing types: Not supported (create new table)
   â””â”€ Deleting fields: Not needed (just stop sending)

Example Schema:
Table: CustomApp_Security_CL
â”œâ”€ TimeGenerated: DateTime (auto, ingestion time)
â”œâ”€ Timestamp: DateTime (event time, custom)
â”œâ”€ Computer: String (hostname)
â”œâ”€ AppName: String (application name)
â”œâ”€ EventID: Double (numeric event ID)
â”œâ”€ EventType: String (Login, Logout, Access, etc.)
â”œâ”€ User: String (UPN or username)
â”œâ”€ SourceIP: String (IP address)
â”œâ”€ Result: String (Success, Failure)
â”œâ”€ Message: String (detailed message)
â””â”€ Severity: Double (1-10 severity scale)
```

### 6.4 Parsing Custom Logs

**KQL Parsing Examples:**

**Example 1: Parse IIS Logs**

```kql
// IIS log format (W3C):
// 2025-10-23 10:30:15 GET /api/users 200 0.5 203.0.113.50

CustomIISLog_CL
| extend ParsedFields = split(RawData_s, ' ')
| extend 
    Date = tostring(ParsedFields[0]),
    Time = tostring(ParsedFields[1]),
    Method = tostring(ParsedFields[2]),
    URI = tostring(ParsedFields[3]),
    StatusCode = toint(ParsedFields[4]),
    ResponseTime = todouble(ParsedFields[5]),
    ClientIP = tostring(ParsedFields[6])
| extend Timestamp = todatetime(strcat(Date, ' ', Time))
| project Timestamp, Method, URI, StatusCode, ResponseTime, ClientIP
```

**Example 2: Parse JSON Logs**

```kql
// JSON log format:
// {"timestamp": "2025-10-23T10:30:15Z", "user": "john@contoso.com", "action": "login", "result": "success"}

CustomAppLog_CL
| extend JsonParsed = parse_json(RawData_s)
| extend
    Timestamp = todatetime(JsonParsed.timestamp),
    User = tostring(JsonParsed.user),
    Action = tostring(JsonParsed.action),
    Result = tostring(JsonParsed.result)
| project Timestamp, User, Action, Result
```

**Example 3: Create Function for Reusability**

```kql
// Save as function: ParseIISLogs()

let ParseIISLogs = () {
    CustomIISLog_CL
    | extend ParsedFields = split(RawData_s, ' ')
    | extend 
        Date = tostring(ParsedFields[0]),
        Time = tostring(ParsedFields[1]),
        Method = tostring(ParsedFields[2]),
        URI = tostring(ParsedFields[3]),
        StatusCode = toint(ParsedFields[4]),
        ResponseTime = todouble(ParsedFields[5]),
        ClientIP = tostring(ParsedFields[6])
    | extend Timestamp = todatetime(strcat(Date, ' ', Time))
    | project Timestamp, Method, URI, StatusCode, ResponseTime, ClientIP
};

// Usage in analytics rules:
ParseIISLogs()
| where StatusCode >= 400  // Client/server errors
| where ResponseTime > 5  // Slow responses (>5 sec)
| summarize count() by URI, StatusCode
```

**ğŸ¯ Exam Tip:**
- **Custom logs** = Any data not covered by built-in connectors
- **HTTP Data Collector API** = REST API for custom log ingestion
- **Table naming**: Name_CL (suffix _CL automatically added)
- **Field types**: _s (string), _d (double), _b (boolean), _t (datetime), _g (GUID)
- **Parsing**: use parse_json(), split(), extract() for structured data
- **Functions**: Save parsed queries as functions for reusability

---

## 7. Threat Intelligence Integration

### 7.1 Threat Intelligence Overview

**What is Threat Intelligence (TI)?**

```
Definition: Information about threats, threat actors, and indicators of compromise (IoCs)

Types of Threat Intelligence:

1ï¸âƒ£ Strategic TI (High-Level)
   â”œâ”€ Audience: Executives, board members
   â”œâ”€ Content: Threat landscape trends, geopolitical risks
   â”œâ”€ Use: Business decisions, risk assessment
   â””â”€ Example: "Ransomware attacks increased 200% in Q3"

2ï¸âƒ£ Tactical TI (Mid-Level)
   â”œâ”€ Audience: Security architects, SOC managers
   â”œâ”€ Content: TTPs (Tactics, Techniques, Procedures)
   â”œâ”€ Use: Detection strategy, security architecture
   â””â”€ Example: "APT28 uses spear-phishing with macro-enabled docs"

3ï¸âƒ£ Operational TI (Action-Oriented)
   â”œâ”€ Audience: SOC analysts, incident responders
   â”œâ”€ Content: Specific threats, attack campaigns
   â”œâ”€ Use: Current threat awareness, incident response
   â””â”€ Example: "CVE-2025-1234 actively exploited in the wild"

4ï¸âƒ£ Technical TI (IoCs - Indicators of Compromise)
   â”œâ”€ Audience: Security tools, SIEM, EDR, firewalls
   â”œâ”€ Content: IPs, domains, file hashes, URLs
   â”œâ”€ Use: Automated detection, blocking
   â””â”€ Example: "IP 203.0.113.50 associated with Emotet C2"
```

### 7.2 Threat Intelligence in Sentinel

**ğŸ†• July 2025: New Threat Intelligence Tables (STIX 2.1)**

```
Major Update: Sentinel TI Architecture Revamp

Old (Legacy):
â””â”€ ThreatIntelligenceIndicator (single table, limited schema)

ğŸ†• New (2025):
â”œâ”€ ThreatIntelIndicators (STIX 2.1 indicators)
â””â”€ ThreatIntelObjects (relationships, threat actors, attack patterns)

ğŸ†• STIX 2.1 Support:
- Structured Threat Information eXpression (STIX)
- Industry standard for threat intel sharing
- Richer data model: Relationships, context, confidence

Migration Timeline:
â”œâ”€ Now: Both old and new tables populated (dual write)
â”œâ”€ July 31, 2025: Deadline to migrate queries, rules, workbooks
â””â”€ August 1, 2025: Legacy table (ThreatIntelligenceIndicator) deprecated

Action Required:
âš ï¸ Update custom queries, analytics rules, workbooks to use new tables
âš ï¸ Test before July 31, 2025 to avoid disruption
```

**ThreatIntelIndicators Table (New):**

```kql
// View schema
ThreatIntelIndicators
| getschema

Key Columns:
â”œâ”€ TimeGenerated: Ingestion time
â”œâ”€ IndicatorId: Unique identifier (GUID)
â”œâ”€ ThreatType: malware, botnet, phishing, etc.
â”œâ”€ ConfidenceScore: 0-100 (confidence in indicator)
â”œâ”€ Active: true/false (is indicator still active?)
â”œâ”€ ExpirationDateTime: When indicator expires
â”œâ”€ Description: Threat description
â”œâ”€ ThreatSeverity: 0-5 (severity scale)
â”‚
â”œâ”€ IoC Fields (Indicator of Compromise):
â”‚  â”œâ”€ NetworkIP: IP address
â”‚  â”œâ”€ NetworkDestinationIP: Destination IP
â”‚  â”œâ”€ Url: URL
â”‚  â”œâ”€ DomainName: Domain
â”‚  â”œâ”€ EmailSourceDomain: Email domain
â”‚  â”œâ”€ EmailSourceIpAddress: Email server IP
â”‚  â”œâ”€ FileHashValue: File hash (MD5, SHA1, SHA256)
â”‚  â””â”€ FileHashType: Hash algorithm
â”‚
â”œâ”€ Source Information:
â”‚  â”œâ”€ SourceSystem: Microsoft Defender TI, TAXII, etc.
â”‚  â”œâ”€ Tags: Custom tags (APT28, ransomware, etc.)
â”‚  â””â”€ AdditionalInformation: JSON with extra context
â”‚
â””â”€ ğŸ†• STIX Fields:
   â”œâ”€ StixId: STIX object ID
   â”œâ”€ PatternType: STIX pattern type
   â””â”€ Pattern: STIX pattern (e.g., IPv4 address)

Example Query (Find threats in network logs):
ThreatIntelIndicators
| where Active == true
| where NetworkIP != ""
| join kind=inner (
    CommonSecurityLog
    | where TimeGenerated > ago(1d)
  ) on $left.NetworkIP == $right.DestinationIP
| project TimeGenerated, SourceIP, DestinationIP, ThreatType, Description
```

**ThreatIntelObjects Table (New - Relationships):**

```kql
// View schema
ThreatIntelObjects
| getschema

Key Columns:
â”œâ”€ TimeGenerated: Ingestion time
â”œâ”€ ObjectId: Unique identifier (STIX ID)
â”œâ”€ ObjectType: threat-actor, attack-pattern, malware, tool, etc.
â”œâ”€ Name: Object name (e.g., "APT28", "Mimikatz")
â”œâ”€ Description: Detailed description
â”œâ”€ Created: Creation timestamp
â”œâ”€ Modified: Last modified timestamp
â”‚
â”œâ”€ ğŸ†• Relationship Support:
â”‚  â”œâ”€ SourceRef: Source object ID
â”‚  â”œâ”€ TargetRef: Target object ID
â”‚  â”œâ”€ RelationshipType: uses, targets, attributed-to, etc.
â”‚  â””â”€ Example: "APT28 uses Mimikatz"
â”‚
â””â”€ Additional Fields: JSON with full STIX 2.1 object

Example Query (Find attack patterns used by threat actor):
ThreatIntelObjects
| where ObjectType == "relationship"
| where SourceRef contains "threat-actor--APT28"
| where RelationshipType == "uses"
| join kind=inner (
    ThreatIntelObjects
    | where ObjectType == "attack-pattern"
  ) on $left.TargetRef == $right.ObjectId
| project ThreatActor = "APT28", AttackPattern = Name, Description
```

### 7.3 Threat Intelligence Sources

**Built-in TI Sources:**

```
ğŸ†• Microsoft Defender Threat Intelligence (MDTI) - FREE (July 2025)
â”œâ”€ Previously: $10K+/year (expensive!)
â”œâ”€ ğŸ†• Now: FREE for all Sentinel + Defender XDR customers
â”œâ”€ Content:
â”‚  â”œâ”€ 84 trillion daily signals (Microsoft global visibility)
â”‚  â”œâ”€ 10,000+ security professionals' analysis
â”‚  â”œâ”€ Threat actor profiles (APT groups, ransomware gangs)
â”‚  â”œâ”€ Vulnerability intelligence (CVEs, exploits)
â”‚  â”œâ”€ IoC feeds (IPs, domains, file hashes) - real-time updates
â”‚  â””â”€ Attack tooling analysis (malware, exploits)
â”‚
â”œâ”€ Rollout: Phase 1 by October 2025, complete by H1 2026
â”œâ”€ Integration: Auto-ingested into ThreatIntelIndicators/Objects tables
â””â”€ ğŸ†• Bi-directional export: Export TI back to TAXII servers (2025)

Microsoft Threat Intelligence (Legacy - Being Replaced by MDTI)
â”œâ”€ Source: Microsoft Security Response Center (MSRC)
â”œâ”€ Content: CVEs, security updates, threat actors
â””â”€ Note: Converging into MDTI (unified experience)

Third-Party TI Platforms (Partner Ecosystem):
â”œâ”€ Commercial: Recorded Future, Anomali, ThreatConnect, CrowdStrike
â”œâ”€ Open-source: AlienVault OTX, MISP, Abuse.ch, VirusTotal
â”œâ”€ Community: ISACs, CERTs, government feeds
â””â”€ Integration: TAXII 2.x, STIX 2.1, upload API, connectors
```

### 7.4 TAXII Connector

**Trusted Automated eXchange of Intelligence Information (TAXII):**

```
What is TAXII?
- Protocol for exchanging threat intelligence
- Standard: TAXII 2.0, TAXII 2.1 (latest)
- Format: STIX (Structured Threat Information eXpression)

TAXII Architecture:
TI Platform (TAXII Server) â†’ Sentinel (TAXII Client)

Configuration Steps:

Step 1: Get TAXII Server Details
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
From TI provider (e.g., AlienVault OTX, Anomali):
â”œâ”€ API Root URL: https://otx.alienvault.com/taxii/
â”œâ”€ Collection ID: collection-12345
â”œâ”€ Username/API Key: (authentication)
â””â”€ Poll interval: How often to fetch (1 hour, 1 day, etc.)

Step 2: Configure TAXII Connector in Sentinel
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Azure Portal â†’ Sentinel â†’ Data connectors â†’ Threat Intelligence - TAXII

Configuration:
â”œâ”€ Friendly name: AlienVault OTX
â”œâ”€ API root URL: https://otx.alienvault.com/taxii/
â”œâ”€ Collection ID: collection-12345
â”œâ”€ Username: your-username
â”œâ”€ Password/API Key: your-api-key
â”œâ”€ Import indicators from: 30 days ago (initial load)
â””â”€ Poll frequency: Once an hour (recommended)

Step 3: Verify Ingestion
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Wait: 5-10 minutes for first indicators

Query:
ThreatIntelIndicators
| where SourceSystem contains "TAXII"
| where Active == true
| summarize count() by ThreatType
| render columnchart

Expected: Indicators appearing with SourceSystem = "TAXII - AlienVault OTX"

Step 4: ğŸ†• Bi-Directional Export (2025)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
New Feature: Export TI from Sentinel back to TAXII servers

Use Case: Share threat intel with partners, community

Configuration:
1. Sentinel â†’ Threat Intelligence â†’ Export
2. Select indicators to export (custom queries)
3. Configure TAXII 2.1 server (destination)
4. Export: Manual or scheduled

Example: Export malicious IPs detected in your environment
ThreatIntelIndicators
| where ThreatType == "botnet"
| where SourceSystem == "Sentinel - Custom Detection"
| where ConfidenceScore > 80
```

### 7.5 STIX Indicator Upload

**Manual Indicator Upload (STIX Format):**

```
Use Case: Upload custom IoCs (from incident response, threat intel reports)

STIX Bundle Example (JSON):
{
  "type": "bundle",
  "id": "bundle--12345",
  "objects": [
    {
      "type": "indicator",
      "id": "indicator--67890",
      "created": "2025-10-23T10:00:00.000Z",
      "modified": "2025-10-23T10:00:00.000Z",
      "name": "Malicious IP from phishing campaign",
      "pattern": "[ipv4-addr:value = '203.0.113.50']",
      "pattern_type": "stix",
      "valid_from": "2025-10-23T10:00:00.000Z",
      "valid_until": "2025-12-31T23:59:59.000Z",
      "labels": ["malicious-activity", "phishing"]
    },
    {
      "type": "indicator",
      "id": "indicator--11111",
      "created": "2025-10-23T10:00:00.000Z",
      "modified": "2025-10-23T10:00:00.000Z",
      "name": "Emotet C2 domain",
      "pattern": "[domain-name:value = 'evil.com']",
      "pattern_type": "stix",
      "valid_from": "2025-10-23T10:00:00.000Z",
      "valid_until": "2025-12-31T23:59:59.000Z",
      "labels": ["malicious-activity", "emotet", "c2"]
    }
  ]
}

Upload Methods:

1ï¸âƒ£ Azure Portal (Manual)
   - Sentinel â†’ Threat Intelligence â†’ Upload indicators
   - Select STIX JSON file
   - Upload (batch upload: up to 10,000 indicators)

2ï¸âƒ£ Upload Indicators API (Programmatic)
   - REST API endpoint: https://management.azure.com/.../uploadIndicators
   - Use for: Automation, integration with custom tools
   - Example: Upload IoCs from incident response tool

3ï¸âƒ£ Logic Apps (Scheduled Upload)
   - Use: Automated periodic uploads
   - Trigger: Schedule (daily, weekly)
   - Action: HTTP request to upload API
   - Source: TI platform, custom database, CSV file
```

### 7.6 Threat Intelligence Matching

**Correlating TI with Logs:**

```kql
// Example 1: Match malicious IPs in firewall logs

// Step 1: Get active malicious IPs from TI
let MaliciousIPs = ThreatIntelIndicators
| where Active == true
| where NetworkIP != ""
| where ConfidenceScore > 70  // High confidence only
| distinct NetworkIP;

// Step 2: Find matches in firewall logs
CommonSecurityLog
| where TimeGenerated > ago(1d)
| where DestinationIP in (MaliciousIPs)
| project TimeGenerated, SourceIP, DestinationIP, DeviceAction
| join kind=inner (ThreatIntelIndicators) on $left.DestinationIP == $right.NetworkIP
| project 
    TimeGenerated, 
    SourceIP, 
    DestinationIP, 
    ThreatType, 
    Description, 
    ConfidenceScore,
    DeviceAction
| order by TimeGenerated desc

// Result: All connections to known malicious IPs

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

// Example 2: Match malicious domains in DNS logs

let MaliciousDomains = ThreatIntelIndicators
| where Active == true
| where DomainName != ""
| distinct DomainName;

DnsEvents
| where TimeGenerated > ago(1d)
| where Name in (MaliciousDomains)
| join kind=inner (ThreatIntelIndicators) on $left.Name == $right.DomainName
| project 
    TimeGenerated, 
    ClientIP, 
    Name, 
    ThreatType, 
    Description

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

// Example 3: Match file hashes (malware detection)

let MaliciousHashes = ThreatIntelIndicators
| where Active == true
| where FileHashValue != ""
| distinct FileHashValue;

DeviceFileEvents  // MDE table
| where TimeGenerated > ago(1d)
| where SHA256 in (MaliciousHashes)
| join kind=inner (ThreatIntelIndicators) on $left.SHA256 == $right.FileHashValue
| project 
    TimeGenerated, 
    DeviceName, 
    FileName, 
    FolderPath, 
    SHA256, 
    ThreatType, 
    Description
```

**Built-in TI Matching (Fusion Rule):**

```
Fusion Rule: Advanced multi-stage attack detection

How It Works:
1. Correlates TI with multiple data sources (automatic)
2. Detects attack chains:
   - TI indicator (malicious IP) + User sign-in
   - Followed by: Unusual file download
   - Followed by: Lateral movement
3. Creates incident (not just alert) - high fidelity

Configuration:
- Sentinel â†’ Analytics â†’ Fusion (ML-powered)
- Enable: Enabled by default (no configuration needed)
- Tuning: Adjust sensitivity (low, medium, high)

Example Fusion Scenario:
1. User signs in from malicious IP (TI match)
2. User downloads unusual file from SharePoint
3. User accesses 50 other user accounts (lateral movement)
4. Fusion creates incident: "Multi-stage attack detected"

Benefit:
âœ… Reduces false positives (correlates multiple weak signals)
âœ… Detects advanced attacks (single indicators may be benign)
âœ… High fidelity (Fusion incidents usually true positives)
```

**ğŸ¯ Exam Tip:**
- ğŸ†• **New TI tables** (2025): ThreatIntelIndicators (STIX 2.1 indicators), ThreatIntelObjects (relationships, threat actors)
- ğŸ†• **Migration deadline**: July 31, 2025 (legacy table deprecated)
- ğŸ†• **MDTI FREE** (July 2025): Microsoft Defender Threat Intelligence included with Sentinel/Defender XDR
- **TAXII**: Protocol for TI exchange (TAXII 2.0/2.1), configure connector for auto-import
- **STIX**: Standard format (JSON), upload via portal or API
- ğŸ†• **Bi-directional export** (2025): Export TI from Sentinel to TAXII servers
- **Matching**: Join TI tables with logs (NetworkIP, DomainName, FileHashValue)
- **Fusion**: ML-powered multi-stage attack detection (correlates TI + behaviors)

---

## 8. Cost Optimization

### 8.1 Understanding Sentinel Costs

**Cost Components:**

```
Microsoft Sentinel Pricing Breakdown:

1ï¸âƒ£ Data Ingestion (Pay-per-GB)
   â”œâ”€ Cost: $2.46/GB (Pay-As-You-Go)
   â”œâ”€ Free tier: First 5 GB/day per workspace (10 GB with Defender for Servers P2)
   â”œâ”€ Commitment tiers: Volume discounts
   â”‚  â”œâ”€ 100 GB/day: $1.20/GB (51% discount)
   â”‚  â”œâ”€ 200 GB/day: $1.10/GB (55% discount)
   â”‚  â”œâ”€ 500 GB/day: $0.98/GB (60% discount)
   â”‚  â””â”€ Up to 5,000 GB/day (custom pricing)
   â””â”€ Note: Charged once (includes Log Analytics + Sentinel)

2ï¸âƒ£ Data Retention (Long-term Storage)
   â”œâ”€ First 90 days: FREE (included)
   â”œâ”€ 91-730 days: $0.10/GB/month (interactive tier)
   â”œâ”€ ğŸ†• Sentinel Data Lake: Lower cost (preview, pricing TBD)
   â””â”€ Archive tier: ~$0.02/GB/month (7+ years, restore on demand)

3ï¸âƒ£ Playbooks (Logic Apps)
   â”œâ”€ Per execution: ~$0.000025 per action
   â”œâ”€ Example: 1,000 playbook runs/month = $0.25/month (negligible)
   â””â”€ Note: Included connectors (no extra cost), premium connectors may charge

4ï¸âƒ£ Notebooks (Azure Machine Learning)
   â”œâ”€ Compute: Charged based on VM usage (when notebook running)
   â”œâ”€ Cost: $0.10-$1.00/hour (depending on VM size)
   â””â”€ Recommendation: Stop compute when not in use

5ï¸âƒ£ Basic Logs (Low-cost Option)
   â”œâ”€ Cost: ~$0.50/GB (80% cheaper than regular logs)
   â”œâ”€ Limitation: Limited KQL queries (no joins, aggregations)
   â”œâ”€ Use: High-volume, low-value logs (verbose logging)
   â””â”€ Tables: Any custom table, some Azure tables

Example Monthly Cost Calculation:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Scenario: 100 GB/day ingestion

Pay-As-You-Go:
- Ingestion: 100 GB/day Ã— 30 days Ã— $2.46/GB = $7,380/month
- Retention (90 days): FREE
- Total: $7,380/month

Commitment Tier (100 GB/day):
- Ingestion: 100 GB/day Ã— 30 days Ã— $1.20/GB = $3,600/month
- Savings: $3,780/month (51% reduction!)

With Extended Retention (1 year):
- Ingestion: $3,600/month
- Retention (91-365 days): 100 GB/day Ã— 275 days Ã— $0.10/GB = $2,750 (one-time)
- Total: $3,600/month + $2,750 retention = $6,350 (first year)
```

### 8.2 Cost Optimization Strategies

**Strategy 1: Commitment Tiers**

```
When to Use:
âœ… Predictable ingestion (>100 GB/day consistent)
âœ… Long-term deployment (not pilot/PoC)

How to Enable:
1. Sentinel â†’ Settings â†’ Pricing
2. Select commitment tier: 100 GB/day, 200 GB/day, etc.
3. Overage: Charged at reduced rate (not Pay-As-You-Go)

Breakeven Analysis:
- 100 GB/day tier costs: 100 GB Ã— $1.20 = $120/day
- Pay-As-You-Go costs: 100 GB Ã— $2.46 = $246/day
- Breakeven: Always beneficial if ingesting â‰¥100 GB/day

Recommendation:
âœ… Use commitment tier if ingestion stable and >100 GB/day
âš ï¸ Review quarterly (can change tier as usage changes)
```

**Strategy 2: Filter Data at Source**

```
Problem: Ingesting too much unnecessary data (noisy events)

Solution: Filter BEFORE ingestion (DCR transformations, XPath)

Example: Windows Security Events (Event ID 5156 - Windows Filtering Platform)
- Volume: 60% of all Security events (very noisy!)
- Value: Low (routine network connections)
- Action: Filter at DCR

Before Filtering:
- Ingestion: 100 GB/day Security events
- Cost: 100 GB Ã— $2.46 = $246/day

After Filtering (Exclude Event ID 5156):
- Ingestion: 40 GB/day (60% reduction!)
- Cost: 40 GB Ã— $2.46 = $98.40/day
- Savings: $147.60/day = $4,428/month (60% reduction!)

DCR Configuration (XPath Filter):
<QueryList>
  <Query Id="0">
    <Select Path="Security">
      *[System[(EventID != 5156 and EventID != 5158 and EventID != 5157)]]
    </Select>
  </Query>
</QueryList>

Other Noisy Events to Consider Filtering:
â”œâ”€ 5156: Windows Filtering Platform (network connections)
â”œâ”€ 5158: Windows Filtering Platform (allowed connection)
â”œâ”€ 5157: Windows Filtering Platform (blocked connection)
â”œâ”€ 4663: Object access (if auditing too verbose)
â””â”€ 4662: Operation performed on object (AD replication, etc.)
```

**Strategy 3: Sample High-Volume Logs**

```
Problem: High-volume logs (IIS, firewall) expensive but only need sample

Solution: Sample data (ingest only X% of logs)

Example: IIS Logs
- Volume: 500 GB/day (detailed request logs)
- Cost: 500 GB Ã— $2.46 = $1,230/day
- Need: Threat detection (not every single request)

Solution: Sample 10% (sufficient for pattern detection)
- Ingestion: 50 GB/day (90% reduction!)
- Cost: 50 GB Ã— $2.46 = $123/day
- Savings: $1,107/day = $33,210/month

Implementation (DCR Transformation):
source
| where rand() < 0.1  // Sample 10% randomly
| project TimeGenerated, ClientIP, URI, StatusCode, UserAgent

âš ï¸ Considerations:
- Works well for: Pattern detection, anomaly detection
- NOT suitable for: Compliance (need 100% logs), forensics
- Recommendation: Sample non-compliance logs, keep compliance logs at 100%
```

**Strategy 4: Basic Logs (Low-Cost Tier)**

```
ğŸ†• Feature: Basic Logs (80% cost reduction)

What: Lower-cost ingestion tier with limited query capabilities

Cost:
- Regular logs: $2.46/GB
- Basic logs: ~$0.50/GB (80% cheaper!)

Limitations:
âŒ No joins (cannot join with other tables)
âŒ No aggregations (summarize, count, avg, etc.)
âŒ No functions (parse_json, extract, etc.)
âœ… Simple filtering only (where, project, take)

When to Use:
âœ… High-volume, low-value logs (verbose application logs)
âœ… Logs rarely queried (archive, compliance)
âœ… Logs used only for specific incident investigation (not analytics)

Example: IIS Logs (500 GB/day)
- Regular logs: 500 GB Ã— $2.46 = $1,230/day
- Basic logs: 500 GB Ã— $0.50 = $250/day
- Savings: $980/day = $29,400/month (80%!)

Configuration:
1. Azure Portal â†’ Log Analytics workspace â†’ Tables
2. Select table: CustomIISLog_CL
3. Change tier: Basic logs
4. Confirm: Data plan changed

Query Limitations Example:
âœ… Allowed:
CustomIISLog_CL
| where StatusCode >= 400
| take 100

âŒ Not Allowed:
CustomIISLog_CL
| where StatusCode >= 400
| summarize count() by URI  // Aggregation not supported
| join kind=inner (ThreatIntelIndicators) on $left.ClientIP == $right.NetworkIP  // Join not supported

Workaround: If need analytics, restore from archive to analytics logs (temporary)
```

**Strategy 5: ğŸ†• Sentinel Data Lake (Preview - July 2025)**

```
ğŸ†• Cost-Effective Long-Term Retention

What: New storage tier for cold data (rarely accessed)

Cost:
- Hot tier (interactive): $0.10/GB/month (after 90 days)
- ğŸ†• Data Lake: Lower cost (pricing TBD, estimated ~$0.03-$0.05/GB/month)
- Archive tier: ~$0.02/GB/month (restore required)

Comparison:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Tier       | Cost/GB/mo | Query | Use Case            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Hot        | $0.10      | Fast  | Active data (90-730d)â”‚
â”‚ Data Lake  | ~$0.04     | Good  | Cold data (compliance)â”‚
â”‚ Archive    | $0.02      | Slow  | Rarely accessed (7y+)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

When to Use:
âœ… Compliance: Long-term retention (2-7 years)
âœ… Cold data: Rarely accessed (not active investigations)
âœ… Cost savings: 60% cheaper than hot tier

Migration Strategy:
1. Identify cold data: Logs >365 days old
2. Migrate: Hot tier â†’ Data Lake (automated policy)
3. Query: Multi-modal analytics (KQL with limitations)
4. Restore: If need full KQL, temporarily restore to hot tier

Configuration:
1. Sentinel â†’ Settings â†’ Data Lake (preview)
2. Enable Data Lake
3. Select tables to migrate
4. Set policy: Move data older than X days to Data Lake

Example Savings:
Scenario: 100 GB/day, 2-year retention

Hot tier only:
- 90 days: FREE
- 91-730 days: 100 GB Ã— 640 days Ã— $0.10/GB = $6,400

With Data Lake:
- 90 days: FREE (hot)
- 91-365 days: 100 GB Ã— 275 days Ã— $0.10/GB = $2,750 (hot)
- 366-730 days: 100 GB Ã— 365 days Ã— $0.04/GB = $1,460 (data lake)
- Total: $4,210 (saving $2,190/year = 34%!)
```

**Strategy 6: Monitor and Optimize Usage**

```
Monitoring Tools:

1ï¸âƒ£ Azure Cost Management
   - Portal: Cost Management + Billing â†’ Cost analysis
   - Filter: Resource type = Log Analytics workspace
   - View: Daily ingestion costs, trends, forecasts
   - Alerts: Set budget alerts (e.g., >$5,000/month)

2ï¸âƒ£ Sentinel Usage Dashboard (Built-in Workbook)
   - Sentinel â†’ Workbooks â†’ Usage
   - View: Ingestion by table, connector, data type
   - Identify: Top contributors (which tables ingesting most?)
   - Action: Optimize high-volume tables

3ï¸âƒ£ KQL Queries for Cost Analysis
   - Query ingestion by table:
Usage
| where TimeGenerated > ago(30d)
| where IsBillable == true
| summarize IngestedGB = sum(Quantity) / 1000 by DataType
| order by IngestedGB desc
| render columnchart

   - Identify noisy tables:
SecurityEvent
| where TimeGenerated > ago(1d)
| summarize count() by EventID
| order by count_ desc
| take 10  // Top 10 noisiest Event IDs

Optimization Process:
1. Identify: High-volume tables (SecurityEvent, Syslog, etc.)
2. Analyze: Is all this data needed? (filtering opportunity?)
3. Optimize: Apply filters, sampling, Basic Logs
4. Monitor: Track ingestion reduction
5. Iterate: Continuous optimization (quarterly reviews)

Typical Optimization Results:
- 30-50% cost reduction (filtering noisy events)
- 60-80% reduction (Basic Logs for verbose logs)
- ROI: Optimization effort pays for itself in 1-2 months
```

**Best Practices Summary:**

```
Cost Optimization Checklist:

âœ… 1. Use commitment tier (if >100 GB/day)
âœ… 2. Filter noisy events at source (DCR, XPath)
âœ… 3. Sample high-volume logs (10-20% sampling)
âœ… 4. Use Basic Logs for verbose, low-value logs
âœ… 5. ğŸ†• Migrate cold data to Data Lake (preview)
âœ… 6. Archive old logs (7+ years for compliance)
âœ… 7. Monitor usage monthly (identify optimization opportunities)
âœ… 8. Review data sources quarterly (disable unused connectors)
âœ… 9. Tune analytics rules (reduce false positives = less investigation time)
âœ… 10. Educate teams (awareness of cost-effective practices)

Expected Savings:
- Tier + filtering: 50-70% cost reduction
- Basic Logs: Additional 50-80% on applicable logs
- Data Lake: 30-60% on long-term retention
- Total: 60-85% overall cost reduction achievable!
```

**ğŸ¯ Exam Tip:**
- **Pricing**: $2.46/GB (Pay-As-You-Go), commitment tiers (51-60% discount)
- **Free tier**: First 5 GB/day per workspace (10 GB with Defender for Servers P2)
- **Retention**: 90 days free, $0.10/GB/month after (hot tier)
- **Optimization strategies**: Commitment tier, filter at source (DCR), sampling, Basic Logs (80% cheaper), ğŸ†• Data Lake (60% cheaper)
- **Monitoring**: Azure Cost Management, Usage workbook, KQL queries (Usage table)
- **Best practice**: Filter noisy Event IDs (5156, 5158, 5157, 4663, 4662)
- **Basic Logs**: Limited queries (no joins, aggregations), 80% cost savings
- ğŸ†• **Sentinel Data Lake** (July 2025): Cost-effective long-term retention, multi-modal analytics

---

**ğŸ‰ END OF MODULE 5 PART 2! ğŸ‰**

You've completed **Sections 4.3-8** covering:
- âœ… Agent-Based Connectors (AMA, Windows Security Events, Linux Syslog)
- âœ… Syslog/CEF Connectors (Network devices, Linux forwarder, CEF parsing)
- âœ… Data Collection Rules (DCR architecture, filtering, transformations)
- âœ… Custom Log Tables (HTTP Data Collector API, schema design, parsing)
- âœ… ğŸ†• Threat Intelligence (2025 updates: ThreatIntelIndicators/Objects, MDTI FREE, bi-directional export)
- âœ… Cost Optimization (commitment tiers, filtering, sampling, Basic Logs, ğŸ†• Data Lake)

**Progress: Module 5 Part 2 of 6 complete!**

**Coming in Part 3:**
- Analytics Rules (Scheduled, NRT, Anomaly, Fusion, Microsoft Security)
- Entity Mapping and MITRE ATT&CK
- ASIM Parsers (Advanced Security Information Model)
- Rule Tuning and Optimization

**Continue to Part 3?** ğŸš€
